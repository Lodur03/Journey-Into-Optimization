{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otimização:  Método dos Gradiente Conjugados\n",
    "\n",
    "*Descrição da Tarefa:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementar o método dos *Gradientes Conjugados* (com o Gram-Schmidt completo e só olhando para a última direção) e utilizá-lo para minimizar funções da forma.\n",
    "\n",
    "$$f(x) = \\sum\\limits_{i=1}^na_i x_i^2$$\n",
    "\n",
    "para diferentes valores de $a_i > 0$\n",
    "\n",
    "### **Funções Auxiliares**\n",
    "\n",
    "Vamos definir algumas funções que serão utilizadas posteriormente para facilitar os cálculos do *método gradiente*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "gradient = lambda x,a: 2*x*a # retorna lista com os gradientes dado ponto (x,y)\n",
    "apply_func = lambda x,a: np.dot(np.square(x), a) # f(x)\n",
    "close_to_zero = lambda grad, tol: np.linalg.norm(grad,2) < tol # stop condition (euclidian distance)\n",
    "\n",
    "def gram_schimidt(grad, a, di):\n",
    "    resp = np.zeros(len(di[0]))\n",
    "    for d in di:\n",
    "        resp += np.dot((np.dot(np.multiply(grad,2*a), d)/np.dot(np.multiply(d,2*a), d)),d)\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função Principal\n",
    "\n",
    "Agora já conseguimos definir a função *conjugated_gradient* (método dos gradientes conjugados). Esta função possui 2 modos de execução:\n",
    "\n",
    "- **Gram-Schimidt Completo:** Para `complete_schimidt=True`, utilizamos o Gram Schimidt completo (todas as direções anteriores)\n",
    "- **Apenas última direção:** Para `complete_schimidt=False`, utilizamos apenas a última direção."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, a, complete_schimidt=True, tolerance=0.01, verbose=False, print_d=False):\n",
    "    \n",
    "    di = []\n",
    "    grad = gradient(x,a)\n",
    "    d = -1*grad\n",
    "    it = 0\n",
    "\n",
    "    while((close_to_zero(grad,tolerance) == False)):\n",
    "        \n",
    "        if(print_d == True):\n",
    "            print('it = ', it, ', d = ', d)\n",
    "            \n",
    "        if(complete_schimidt == True):\n",
    "            di.append(d)\n",
    "        else:\n",
    "            di = [d]\n",
    "            \n",
    "        alpha = -1*(np.dot(grad,d))/(np.dot(np.multiply(d,2*a), d))\n",
    "        x = x + (d*alpha)\n",
    "        grad = gradient(x,a)\n",
    "        d = -1*grad + gram_schimidt(grad, a, di)\n",
    "        it = it + 1\n",
    "        \n",
    "    if(verbose == True):\n",
    "        print('Iteracoes utilizadas = ', it)\n",
    "        \n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diferença das Direções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it =  0 , d =  [ -0.  -2.  -8. -18. -32. -50.]\n",
      "it =  1 , d =  [ 0.         -1.60894864 -4.66585106 -6.51579201 -4.50385624  4.02487152]\n",
      "it =  2 , d =  [ 0.         -1.19376771 -2.00525116 -0.66959609  1.39155136 -0.42995533]\n",
      "it =  3 , d =  [ 0.         -0.76359946 -0.37058269  0.49909561 -0.23441844  0.04204368]\n",
      "it =  4 , d =  [ 0.         -0.36234855  0.18117427 -0.0805219   0.02264678 -0.00289879]\n",
      "Iteracoes utilizadas =  5\n",
      "Ponto de mínimo encontrado:  [ 0.00000000e+00 -2.77555756e-17  0.00000000e+00  7.28583860e-17\n",
      "  1.20563282e-16  3.26670115e-16]\n"
     ]
    }
   ],
   "source": [
    "a = np.float64(range(6)) # temos, por exemplo, esses valores de a > 0\n",
    "x = np.float64(range(6)) # e um chute inicial dos valores de xi\n",
    "\n",
    "convergence_x = gradient_descent(x, a, complete_schimidt=True, tolerance=0.001, verbose=True, print_d=True)\n",
    "print(\"Ponto de mínimo encontrado: \", convergence_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it =  0 , d =  [ -0.  -2.  -8. -18. -32. -50.]\n",
      "it =  1 , d =  [ 0.         -1.60894864 -4.66585106 -6.51579201 -4.50385624  4.02487152]\n",
      "it =  2 , d =  [ 0.         -1.19376771 -2.00525116 -0.66959609  1.39155136 -0.42995533]\n",
      "it =  3 , d =  [ 0.         -0.76359946 -0.37058269  0.49909561 -0.23441844  0.04204368]\n",
      "it =  4 , d =  [ 0.         -0.36234855  0.18117427 -0.0805219   0.02264678 -0.00289879]\n",
      "Iteracoes utilizadas =  5\n",
      "Ponto de mínimo encontrado:  [ 0.00000000e+00 -2.77555756e-17 -4.16333634e-17  6.93889390e-18\n",
      " -9.71445147e-17 -3.62340366e-16]\n"
     ]
    }
   ],
   "source": [
    "a = np.float64(range(6)) # temos, por exemplo, esses valores de a > 0\n",
    "x = np.float64(range(6)) # e um chute inicial dos valores de xi\n",
    "\n",
    "convergence_x = gradient_descent(x, a, complete_schimidt=False, tolerance=0.001, verbose=True, print_d=True)\n",
    "print(\"Ponto de mínimo encontrado: \", convergence_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Análise:** Podemos observar que, para este exemplo, as direções encontradas com o gram-schimdt completo e utilizando apenas a última direção são bem similares, resultando na convergência em um mesmo número de iterações. Na seção de análises adicionais mostramos que há contra-exemplos, com configurações específicas dos coeficientes da função (que implicam em diferentes autovalores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparação das Funções\n",
    "\n",
    "**i)** $f(x_1, x_2) = 1x_1^2 + 1.1x_2^2$\n",
    "\n",
    "- Com método completo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteracoes utilizadas =  2\n",
      "Ponto de mínimo encontrado:  [-2.77555756e-17 -4.85722573e-17]\n"
     ]
    }
   ],
   "source": [
    "a = np.float64([1, 1.1]) # temos, por exemplo, esses valores de a > 0\n",
    "x = np.float64([1,2]) # e um chute inicial dos valores de xi\n",
    "\n",
    "convergence_x = gradient_descent(x, a, complete_schimidt=True, tolerance=0.001, verbose=True)\n",
    "print(\"Ponto de mínimo encontrado: \", convergence_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apenas última direção:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteracoes utilizadas =  2\n",
      "Ponto de mínimo encontrado:  [-2.77555756e-17 -4.85722573e-17]\n"
     ]
    }
   ],
   "source": [
    "a = np.float64([1, 1.1]) # temos, por exemplo, esses valores de a > 0\n",
    "x = np.float64([1,2]) # e um chute inicial dos valores de xi\n",
    "\n",
    "convergence_x = gradient_descent(x, a, complete_schimidt=False, tolerance=0.001, verbose=True)\n",
    "print(\"Ponto de mínimo encontrado: \", convergence_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Análise:** Podemos observar que ambos os experimentos, neste caso, levam para exatamente o mesmo valor utilizando exatamente o mesmo número de iterações."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ii)** $f(x_1, x_2) = 1x_1^2 + 100x_2^2$\n",
    "\n",
    "- Com método completo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteracoes utilizadas =  2\n",
      "Ponto de mínimo encontrado:  [0.00000000e+00 1.79462565e-16]\n"
     ]
    }
   ],
   "source": [
    "a = np.float64([1, 100]) # temos, por exemplo, esses valores de a > 0\n",
    "x = np.float64([1,2]) # e um chute inicial dos valores de xi\n",
    "\n",
    "convergence_x = gradient_descent(x, a, complete_schimidt=True, tolerance=0.001, verbose=True)\n",
    "print(\"Ponto de mínimo encontrado: \", convergence_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Usando apenas última direção:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteracoes utilizadas =  2\n",
      "Ponto de mínimo encontrado:  [0.00000000e+00 1.79462565e-16]\n"
     ]
    }
   ],
   "source": [
    "a = np.float64([1, 100]) # temos, por exemplo, esses valores de a > 0\n",
    "x = np.float64([1,2]) # e um chute inicial dos valores de xi\n",
    "\n",
    "convergence_x = gradient_descent(x, a, complete_schimidt=False, tolerance=0.001, verbose=True)\n",
    "print(\"Ponto de mínimo encontrado: \", convergence_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Análise:**  Podemos observar que ambos os experimentos, neste caso, levam para exatamente o mesmo valor utilizando exatamente o mesmo número de iterações. Em particular, sabemos que isso ocorre pois, como apenas duas iterações são necessárias, ambos os métodos apenas utilizam uma direção para atualizar os valores e, portanto, para detectar diferenças entre os valores gerados por cada método seria necessário apresentar exemplos que necessitam de mais iterações para convergir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análises Adicionais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É importante comparar a eficiência de cada implementação, apesar de obter iterações semelhantes, pode haver uma diferença no tempo de execução. Em particular, temos a hipótese de que o gram-schimdt completo seria menos computacionalmente eficiente. Para confirmar essa hipótese, conduzimos um experimento simples no qual medimos o tempo de execução em cada um.\n",
    "\n",
    "**Para Gram-Schimdt Completo:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de execução de aproximadamente 0.0246 segundos\n"
     ]
    }
   ],
   "source": [
    "a = np.float64(range(1,100)) # temos, por exemplo, esses valores de a > 0\n",
    "x = np.float64(range(1,100)) # e um chute inicial dos valores de xi\n",
    "\n",
    "start_time = time.clock()\n",
    "convergence_x = gradient_descent(x, a, complete_schimidt=True, tolerance=0.001, verbose=False)\n",
    "total_time = time.clock() - start_time\n",
    "print(\"Tempo de execução de aproximadamente\", round(total_time,4), 'segundos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usando apenas o última Direção:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de execução de aproximadamente 0.0072 segundos\n"
     ]
    }
   ],
   "source": [
    "a = np.float64(range(1,100)) # temos, por exemplo, esses valores de a > 0\n",
    "x = np.float64(range(1,100)) # e um chute inicial dos valores de xi\n",
    "\n",
    "start_time = time.clock()\n",
    "convergence_x = gradient_descent(x, a, complete_schimidt=False, tolerance=0.001, verbose=False)\n",
    "total_time = time.clock() - start_time\n",
    "print(\"Tempo de execução de aproximadamente\", round(total_time,4), 'segundos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Análise:** Podemos observar que há uma diferença considerável no tempo de execução entre os métodos. Em particular, nota-se que o método utilizando apenas a última direção é muito mais rápido (0.005s contra os 0.022s do método completo), além de também economizar memória por não precisar armazenar todas as direções calculadas anteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusters de Autovalores\n",
    "\n",
    "Na procura de configurações da função que resultassem em alguma diferença de iterações para obter convergência com cada um dos métodos, executamos os seguintes experimentos:\n",
    "- Gram Schimdt Completo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteracoes utilizadas =  5\n",
      "Ponto de mínimo encontrado:  [ 2.41776204e-05 -7.99260179e-05  1.00740085e-04  6.16776031e-06\n",
      "  6.16776031e-06 -5.72251371e-05  9.25988880e-22]\n"
     ]
    }
   ],
   "source": [
    "a = np.float64([1,1.1,1.2,1.4,1.4,1.3,100000000000]) # temos, por exemplo, esses valores de a > 0\n",
    "x = np.float64([1,1,1,1,1,1,1]) # e um chute inicial dos valores de xi\n",
    "\n",
    "convergence_x = gradient_descent(x, a, complete_schimidt=True, tolerance=0.001, verbose=True, print_d=False)\n",
    "print(\"Ponto de mínimo encontrado: \", convergence_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apenas última direção:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteracoes utilizadas =  7\n",
      "Ponto de mínimo encontrado:  [ 2.41776221e-05 -7.99260182e-05  1.00740084e-04  6.16776095e-06\n",
      "  6.16776095e-06 -5.72251382e-05  4.41631729e-21]\n"
     ]
    }
   ],
   "source": [
    "a = np.float64([1,1.1,1.2,1.4,1.4,1.3,100000000000]) # temos, por exemplo, esses valores de a > 0\n",
    "x = np.float64([1,1,1,1,1,1,1]) # e um chute inicial dos valores de xi\n",
    "\n",
    "convergence_x = gradient_descent(x, a, complete_schimidt=False, tolerance=0.001, verbose=True, print_d=False)\n",
    "print(\"Ponto de mínimo encontrado: \", convergence_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Análise:** Podemos observar que, ao estabelecer um conjunto de autovalores próximos (cluster) e outro(s) autovalor(es) bem separados, os resultados obtidos pelos dois métodos apresentam certa diferença. Em particular, nota-se que que o método completo é capaz de realizar menos iterações enquanto o método que se utiliza apenas a última direção necessita de mais iterações para convergir. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
